{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of a Generative Adversarial Network\n",
    "\n",
    "\n",
    "This notebook shows hot to build and train an adversarial generative network to train 2D images. \n",
    "\n",
    "We are looking at the 2D projection of some 3D calorimetric images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "import h5py \n",
    "import numpy as np\n",
    "\n",
    "#options for GPU running\n",
    "import tensorflow as tf\n",
    "session_config = tf.ConfigProto(log_device_placement=True)\n",
    "session_config.gpu_options.allow_growth=True\n",
    "session = tf.Session(config=session_config)\n",
    "K.set_session(session)\n",
    "\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adadelta, Adam, RMSprop\n",
    "from keras.utils.generic_utils import Progbar\n",
    "from sklearn.model_selection import train_test_split\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import (Input, Dense, Reshape, Flatten, Lambda, merge,\n",
    "                          Dropout, BatchNormalization, Activation, Embedding)\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import (UpSampling2D, Conv2D, ZeroPadding2D,\n",
    "                                        AveragePooling2D)\n",
    "\n",
    "from keras.models import Model, Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 20\n",
    "batch_size = 1000\n",
    "latent_size = 100\n",
    "nevt = 50000   # number of events\n",
    "verbose = 'false'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator model\n",
    "\n",
    "Create discriminator model using Keras functional model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = Input(shape=(1, 25, 25))\n",
    "\n",
    "x = Conv2D(32, (5,5), data_format='channels_first', padding='same')(input_image)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = ZeroPadding2D((2,2))(x)\n",
    "x = Conv2D(8, (5, 5), data_format='channels_first', padding='valid')(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = ZeroPadding2D((2, 2))(x)\n",
    "x = Conv2D(8, (5,5), data_format='channels_first', padding='valid')(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = ZeroPadding2D((1, 1))(x)\n",
    "x = Conv2D(8, (5, 5), data_format='channels_first', padding='valid')(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "\n",
    "x = AveragePooling2D((2, 2))(x)\n",
    "h = Flatten()(x)\n",
    "\n",
    "#dnn = Model(input_image, h)\n",
    "#print('Discriminator Model summary')\n",
    "#dnn.summary()\n",
    "\n",
    "#image = Input(shape=(1, 25, 25))\n",
    "\n",
    "#dnn_out = dnn(image)\n",
    "\n",
    "\n",
    "fake = Dense(1, activation='sigmoid')(h)\n",
    "\n",
    "# to add extra loss functions\n",
    "#    aux = Dense(1, activation='linear', name='auxiliary')(dnn_out)\n",
    "#    ecal = Lambda(lambda x: K.sum(x, axis=(2, 3)))(image)\n",
    "    \n",
    "discriminator = Model(input=input_image, output=[fake])\n",
    "discriminator.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Sequential()\n",
    "\n",
    "latent_size = 1024\n",
    "gen.add(Dense(64 * 7, input_dim=latent_size))\n",
    "gen.add(Reshape((8, 7,8)))\n",
    "\n",
    "gen.add(Conv2D(64, (6, 8), data_format='channels_first', padding='same', kernel_initializer='he_uniform'))\n",
    "gen.add(LeakyReLU())\n",
    "gen.add(BatchNormalization())\n",
    "\n",
    "gen.add(UpSampling2D(size=(2, 2)))\n",
    "gen.add(ZeroPadding2D((2, 0)))\n",
    "\n",
    "gen.add(Conv2D(6, (5, 8), data_format='channels_first', kernel_initializer='he_uniform'))\n",
    "gen.add(LeakyReLU())\n",
    "gen.add(BatchNormalization())\n",
    "\n",
    "gen.add(UpSampling2D(size=(2, 3)))\n",
    "gen.add(ZeroPadding2D((0,3)))\n",
    "\n",
    "gen.add(Conv2D(6, (3, 8), data_format='channels_first', kernel_initializer='he_uniform'))\n",
    "gen.add(LeakyReLU())\n",
    "\n",
    "gen.add(Conv2D(1, (2, 2), data_format='channels_first', use_bias=False, kernel_initializer='glorot_normal'))\n",
    "gen.add(Activation('relu'))\n",
    "    \n",
    "generator = gen\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('[INFO] Building discriminator')\n",
    "    #discriminator.load_weights('veganweights/params_discriminator_epoch_019.hdf5')\n",
    "discriminator.compile(optimizer=RMSprop(), loss='binary_crossentropy')\n",
    "\n",
    "discriminator.summary()\n",
    "\n",
    "# build the generator\n",
    "print('[INFO] Building generator')\n",
    "#generator.load_weights('veganweights/params_generator_epoch_019.hdf5')\n",
    "generator.compile( optimizer=RMSprop(), loss='binary_crossentropy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = Input(shape=(latent_size, ), name='combined_z')\n",
    "\n",
    "fake_image = generator( latent)\n",
    "\n",
    "discriminator.trainable = False\n",
    "fake_disc_out = discriminator(fake_image)\n",
    "combined = Model(inputs=[latent], outputs=[fake_disc_out], name='combined_model')\n",
    "\n",
    "combined.compile( optimizer=RMSprop(), loss='binary_crossentropy')\n",
    "combined.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Full data set (~ 20 GB)\n",
    "##d=h5py.File(\"/home/moneta/data/Ele_v1_1_2.h5\",'r')\n",
    "d=h5py.File(\"Ele_GAN_2D.h5\",'r')\n",
    "    #  target is 200k x 2\n",
    "    #  ECAL is 200k x 25 x 25 x25\n",
    "e=d.get('target')\n",
    "xd = d.get('ECAL')\n",
    "print(xd.shape, e.shape)\n",
    "\n",
    "print ('Number of events in file',xd.shape[0])\n",
    "\n",
    "print('image size is :',xd.shape[2],' x ',xd.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert in input Numpy array\n",
    "X=np.array(xd[:nevt,:,:,:])\n",
    "y=(np.array(e[:nevt]))\n",
    "\n",
    "print('*** Input data shapes ***')\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define variables before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unphysical values\n",
    "X[X < 1e-6] = 0\n",
    "\n",
    "# split in train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2)\n",
    "\n",
    "\n",
    "y_train= y_train/100\n",
    "y_test=y_test/100\n",
    "\n",
    "\n",
    "nb_train, nb_test = X_train.shape[0], X_test.shape[0]\n",
    "\n",
    "#converte to float32\n",
    "\n",
    "X_train = X_train.astype(np.float32)  \n",
    "X_test = X_test.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "ecal_train = np.sum(X_train, axis=(2, 3))\n",
    "ecal_test = np.sum(X_test, axis=(2, 3))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(ecal_train.shape)\n",
    "print(ecal_test.shape)\n",
    "\n",
    "print('*************************************************************************************')\n",
    "\n",
    "#from collections import defaultdict\n",
    "#train_history = defaultdict(list)\n",
    "#test_history = defaultdict(list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "hl_disc_test = ROOT.TH1D(\"hldtest\",\"Loss Discriminator test\",nb_epochs,0,nb_epochs)\n",
    "hl_disc_train = ROOT.TH1D(\"hldtrain\",\"Loss Discriminator training\",nb_epochs,0,nb_epochs)\n",
    "hl_gen_test = ROOT.TH1D(\"hlgtest\",\"Loss Generatator test\",nb_epochs,0,nb_epochs)\n",
    "hl_gen_train = ROOT.TH1D(\"hlgtrain\",\"Loss Generatator train\",nb_epochs,0,nb_epochs)\n",
    "\n",
    "htrue = ROOT.TH2D(\"htrue\",\"true image\",25,0,25,25,0,25)\n",
    "htrueX = ROOT.TH1D(\"htruex\",\"true image in X\",25,0,25)\n",
    "htrueY = ROOT.TH1D(\"htruey\",\"true image in Y\",25,0,25)\n",
    "htrueE = ROOT.TH1D(\"htrueE\",\"true tot energy\",100,0,10)\n",
    "\n",
    "nevents = X_test.shape[0]\n",
    "for index in range(nevents):\n",
    "    sum = 0\n",
    "    for i in range(25):\n",
    "            for j in range(25) :\n",
    "                htrue.Fill(i+.5,j+.5,X_test[index,0,i,j])\n",
    "                htrueX.Fill(i+.5,X_test[index,0,i,j])\n",
    "                htrueY.Fill(j+.5,X_test[index,0,i,j])\n",
    "                sum += X_test[index,0,i,j]\n",
    "    htrueE.Fill(sum)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cname = \"canvas_image\"\n",
    "c1 = ROOT.TCanvas(cname,cname,1500,500)\n",
    "c1.Divide(3,1)\n",
    "c1.cd(1)\n",
    "htrue.SetMaximum(1.2*htrue.GetMaximum())\n",
    "htrue.Draw(\"COLZ\")\n",
    "c1.cd(2)\n",
    "htrueX.Draw('HIST')\n",
    "c1.cd(3)\n",
    "htrueY.Draw('HIST')\n",
    "c1.Draw()\n",
    "            \n",
    "closs = ROOT.TCanvas(\"closs\",\"closs\",1200,1200)\n",
    "closs.Divide(2,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    print('Epoch ',epoch + 1,' of ', nb_epochs)\n",
    "\n",
    "    nb_batches = int(X_train.shape[0] / batch_size)\n",
    "    if verbose:\n",
    "        progress_bar = Progbar(target=nb_batches)\n",
    "\n",
    "    epoch_gen_loss = []\n",
    "    epoch_disc_loss = []\n",
    "    for index in range(nb_batches):\n",
    "        if verbose:\n",
    "            progress_bar.update(index)\n",
    "        else:\n",
    "            if index % 1 == 0:\n",
    "                print('processed {}/{} batches'.format(index + 1, nb_batches))\n",
    "\n",
    "       #generate random noise\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_size))\n",
    "\n",
    "        image_batch = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "        energy_batch = y_train[index * batch_size:(index + 1) * batch_size]\n",
    "        \n",
    "        \n",
    "        #generate random data for generator\n",
    "        sampled_energies = np.random.uniform(1, 5,( batch_size,1 ))\n",
    "        generator_input = np.multiply(sampled_energies, noise)\n",
    "        \n",
    "        # perform generator prediction\n",
    "        generated_images = generator.predict(generator_input, verbose=0)\n",
    "\n",
    "        # train discriminator on real batch and fake batch \n",
    "        # real batch will have label 1 and fake label 0\n",
    "        real_batch_loss = discriminator.train_on_batch(image_batch, np.ones(batch_size)) \n",
    "        fake_batch_loss = discriminator.train_on_batch(generated_images, np.zeros(batch_size)) \n",
    "  \n",
    "        # compute total discriminator loss \n",
    "        discrim_loss = (real_batch_loss + fake_batch_loss)/2\n",
    "        \n",
    "        #print(discrim_loss)\n",
    "        \n",
    "        # save loss for each epoch\n",
    "        epoch_disc_loss.append(discrim_loss)\n",
    "\n",
    "\n",
    "\n",
    "        #generate input random to generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_size))\n",
    "        sampled_energies = np.random.uniform(1, 5, ( batch_size,1 ))\n",
    "        generator_input = np.multiply(sampled_energies, noise)\n",
    "\n",
    "\n",
    "        # train generator and compute its loss\n",
    "        generator_loss = combined.train_on_batch( generator_input, np.ones(batch_size))\n",
    "        \n",
    "        #print(generator_loss)\n",
    "\n",
    "\n",
    "        epoch_gen_loss.append( generator_loss )\n",
    "                              \n",
    "        #### TESTING                      \n",
    "\n",
    "    print('\\nTesting for epoch :' , epoch + 1)\n",
    "\n",
    "    ### generate test random data                      \n",
    "    noise = np.random.normal(0, 1, (nb_test, latent_size))\n",
    "    sampled_energies = np.random.uniform(1, 5, (nb_test, 1))\n",
    "    generator_input = np.multiply(sampled_energies, noise)\n",
    "    generated_images = generator.predict(generator_input, verbose=False)\n",
    "\n",
    "\n",
    "                              \n",
    "    X = np.concatenate((X_test, generated_images))\n",
    "    y = np.array([1] * nb_test + [0] * nb_test)\n",
    "                              \n",
    "    discriminator_test_loss = discriminator.evaluate(X, y, verbose=False, batch_size=batch_size)\n",
    "\n",
    "     \n",
    "    discriminator_train_loss = np.mean(np.array(epoch_disc_loss), axis=0)\n",
    "\n",
    "    noise = np.random.normal(0, 1, (2 * nb_test, latent_size))\n",
    "    sampled_energies = np.random.uniform(1, 5, (2 * nb_test, 1))\n",
    "    generator_input = np.multiply(sampled_energies, noise)\n",
    "      \n",
    "\n",
    "    trick = np.ones(2 * nb_test)\n",
    "\n",
    "    generator_test_loss = combined.evaluate(generator_input, trick, verbose=False, batch_size=batch_size)\n",
    "\n",
    "    generator_train_loss = np.mean(np.array(epoch_gen_loss), axis=0)\n",
    "\n",
    "\n",
    "    print('generator  train and test loss', generator_train_loss, generator_test_loss)\n",
    "    print('discrimin. train and test loss', discriminator_train_loss, discriminator_test_loss)\n",
    "\n",
    "        \n",
    "        \n",
    "    # save loss in an histogram \n",
    "                              \n",
    "    hl_disc_test.SetBinContent(epoch+1,discriminator_test_loss)\n",
    "    hl_disc_train.SetBinContent(epoch+1,discriminator_train_loss)\n",
    "    hl_gen_test.SetBinContent(epoch+1,generator_test_loss)\n",
    "    hl_gen_train.SetBinContent(epoch+1,generator_train_loss)\n",
    "\n",
    "    # save weights every epoch\n",
    "    generator.save_weights('generator_weights_epoch_'+ str(epoch)+'.h5', overwrite=True)\n",
    "    discriminator.save_weights('discriminator_weights_epoch_'+ str(epoch)+'.h5', overwrite=True)\n",
    "  \n",
    "\n",
    "        # make histograms of generated images\n",
    "        \n",
    "    hname = \"hgen_\" + str(epoch+1)\n",
    "    htitle = \"Generated image_\" + str(epoch+1)\n",
    "    hgen = ROOT.TH2D(hname,htitle,25,0,25,25,0,25)\n",
    "    hgenX = ROOT.TH1D(\"hgenX\"+str(epoch+1),htitle,25,0,25)\n",
    "    hgenY = ROOT.TH1D(\"hgenY\"+str(epoch+1),htitle,25,0,25)\n",
    "    hgenE = ROOT.TH1D(\"hgenE\"+str(epoch+1),htitle,100,0,10)\n",
    "    hgenX.SetLineColor(ROOT.kRed)\n",
    "    hgenY.SetLineColor(ROOT.kRed)\n",
    "    hgenE.SetLineColor(ROOT.kRed)\n",
    "        \n",
    "\n",
    "    nevents = X_test.shape[0]\n",
    "    for index in range(nevents):\n",
    "        sum = 0\n",
    "        for i in range(25):\n",
    "            for j in range(25) :\n",
    "                if (epoch == 0) : htrue.Fill(i+.5,j+.5,X_test[index,0,i,j])\n",
    "                hgen.Fill(i+.5,j+.5,generated_images[index,0,i,j])\n",
    "                hgenX.Fill(i+.5,generated_images[index,0,i,j])\n",
    "                hgenY.Fill(j+.5,generated_images[index,0,i,j])\n",
    "                sum += generated_images[index,0,i,j]\n",
    "                    \n",
    "        hgenE.Fill(sum)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot Loss\n",
    "\n",
    "closs.cd(1)\n",
    "hl_disc_test.Draw()\n",
    "closs.cd(2)\n",
    "hl_disc_train.Draw()\n",
    "closs.cd(3)\n",
    "hl_gen_test.Draw()\n",
    "closs.cd(4)\n",
    "hl_gen_train.Draw()\n",
    "closs.Draw()\n",
    "closs.Update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot images \n",
    "c1 = ROOT.TCanvas(cname,cname,1500,1000)\n",
    "c1.Divide(3,2)\n",
    "c1.cd(1)\n",
    "htrue.SetMaximum(1.2*htrue.GetMaximum())\n",
    "htrue.Draw(\"COLZ\")\n",
    "c1.cd(2)\n",
    "hgen.SetMaximum(1.2*htrue.GetMaximum() )\n",
    "hgen.DrawCopy(\"COLZ\")\n",
    "c1.cd(3)\n",
    "hgenX.SetMaximum(htrueX.GetMaximum()*1.3)\n",
    "hgenX.DrawCopy(\"HIST\")\n",
    "htrueX.DrawCopy(\"SAME HIST\")\n",
    "c1.cd(4)\n",
    "hgenY.SetMaximum(htrueY.GetMaximum()*1.3)\n",
    "hgenY.DrawCopy(\"HIST\")\n",
    "htrueY.DrawCopy(\"SAME HIST\")\n",
    "c1.cd(5)\n",
    "hgenE.SetMaximum(htrueE.GetMaximum()*1.3)\n",
    "hgenE.DrawCopy(\"HIST\")\n",
    "htrueE.DrawCopy(\"SAME\")\n",
    "c1.Update()\n",
    "c1.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
