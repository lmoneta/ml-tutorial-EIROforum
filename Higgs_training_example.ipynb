{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing the results of the paper *Searching for Exotic Particles in High-Energy Physics with Deep Learning*\n",
    "\n",
    "The paper *Searching for Exotic Particles in High-Energy Physics with Deep Learning* by Baldi et al. is one of the most popular papers presenting the successful usage of deep neural networks in high-energy particle physics applications.\n",
    "\n",
    "This example reproduces this important result with only about 100 lines of code using Keras.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import subprocess\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, advanced_activations, Dropout\n",
    "from keras import callbacks\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset\n",
    "\n",
    "This can take a while! The final dataset has a size of about 1.2 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if not os.path.exists(\"HIGGS_small.h5\"):\n",
    "#    subprocess.call(\"wget http://mlphysics.ics.uci.edu/data/higgs/HIGGS_small.h5\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if not os.path.exists(\"HIGGS.h5\"):\n",
    "#    subprocess.call(\"wget http://mlphysics.ics.uci.edu/data/higgs/HIGGS.h5\", shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read-out the inputs and targets\n",
    "\n",
    "The inputs consist of 21 low-level and and 7 high-level variables. We want to reproduce the result of the paper with all features as inputs called `lo+hi-level` in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ = h5py.File(\"HIGGS_data_small.h5\")\n",
    "inputs = np.array(file_[\"features\"][:,:])\n",
    "targets = np.array(file_[\"targets\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"input data\",inputs.shape)\n",
    "print(inputs[1:5])\n",
    "print(\"target data \",targets.shape)\n",
    "print(targets[1:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the models\n",
    "\n",
    "The model defined below do not match exactly the setup in the paper. However, we define a shallow neural network with a single hidden layer and a deep neural network with 5 hidden layers.\n",
    "\n",
    "### Shallow network\n",
    "  \n",
    "we use only one hidden layer with 500 units and tanh activation function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_shallow = Sequential()\n",
    "model_shallow.add(Dense(500, kernel_initializer=\"glorot_normal\", activation=\"tanh\",\n",
    "    input_dim=inputs.shape[1]))\n",
    "model_shallow.add(Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "\n",
    "model_shallow.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep network\n",
    "\n",
    "5 hidden layers with 200 units and RELU activation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deep = Sequential()\n",
    "\n",
    "model_deep.add(Dense(200, kernel_initializer=\"glorot_normal\", activation=\"relu\" ,input_dim=inputs.shape[1]))\n",
    "model_deep.add(Dense(200, kernel_initializer=\"glorot_normal\", activation=\"relu\"))\n",
    "model_deep.add(Dense(200, kernel_initializer=\"glorot_normal\", activation=\"relu\"))\n",
    "model_deep.add(Dense(200, kernel_initializer=\"glorot_normal\", activation=\"relu\"))\n",
    "model_deep.add(Dense(200, kernel_initializer=\"glorot_normal\", activation=\"relu\"))\n",
    "\n",
    "model_deep.add(Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "\n",
    "model_deep.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile then model by defining the loss function and the type of optimizer we are going to use. \n",
    "In this case we use *Nadam*, the Nesterov Adam optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [model_shallow, model_deep]:\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=\"nadam\",\n",
    "        metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training and test data\n",
    "\n",
    "To speed up the training, we use only a reduce number of all the data set. We split then the data in traiing and test data, using 20% for testing and the remaining fpr training.\n",
    "Feel free to enlarge the amount of data to be used. \n",
    "The dataset contains in total ~ 10M events. We use only now 100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of events \n",
    "ntotal_evts = inputs.shape[0]\n",
    "nused_evts = 1000000\n",
    "evtoffset = 0\n",
    "print(\"using for test and training\",nused_evts,\" of a total \",ntotal_evts)\n",
    "\n",
    "inputs_train, inputs_test, targets_train, targets_test = train_test_split(\n",
    "        inputs[evtoffset:evtoffset+nused_evts], targets[evtoffset:evtoffset+nused_evts], test_size=0.10, random_state=1234, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare pre-processing\n",
    "\n",
    "As preprocessing, we use a standard scaler provided by the `sklearn` package. This preprocessing method takes each input and subtracts the mean and then divides by the standard-deviation so that the final distribution is centered around 0 with a width of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_input = StandardScaler()\n",
    "preprocessing_input.fit(inputs_train)\n",
    "pickle.dump(preprocessing_input, open(\"HIGGS_preprocessing.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the models\n",
    "\n",
    "The following code trains the models. Here, you can experience quickly why deep-learning is heavily dependent on GPUs to speed up the training!\n",
    "During training we use 25% of the data for the validation and 75% for the actual training.\n",
    "We define here also the batch size (e.g. 512).\n",
    "At the end we save the model in case we need to reuse later for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, name in zip([model_shallow, model_deep], [\"HIGGS_model_shallow.h5\", \"HIGGS_model_deep.h5\"]):\n",
    "    print(\"\\nTrain now the model \",name)\n",
    "    model.fit(\n",
    "            preprocessing_input.transform(inputs_train),\n",
    "            targets_train,\n",
    "            batch_size=512,\n",
    "            epochs=10,\n",
    "            validation_split=0.25,\n",
    "            callbacks=[callbacks.ModelCheckpoint(name, monitor = 'val_loss', verbose=True,\n",
    "                                                save_best_only=True, mode='auto')]\n",
    "    )\n",
    "    \n",
    "   \n",
    "    #model.save(modelFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the models\n",
    "\n",
    "We load the models and the pre-processing parameters to apply to the previously defined test data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_shallow = load_model(\"HIGGS_model_shallow.h5\")\n",
    "model_deep = load_model(\"HIGGS_model_deep.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_input = pickle.load(open(\"HIGGS_preprocessing.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model prediction using the test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs_test.shape[0])\n",
    "\n",
    "ntest_evts = inputs_test.shape[0]\n",
    "#in case we want to use a smaller set of test data \n",
    "#num_events = 200000\n",
    "predictions_shallow = model_shallow.predict(\n",
    "        preprocessing_input.transform(inputs_test[:ntest_evts]))\n",
    "predictions_deep = model_deep.predict(\n",
    "        preprocessing_input.transform(inputs_test[:ntest_evts]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute ROC curve and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_shallow, tpr_shallow, _ = roc_curve(targets_test[:ntest_evts], predictions_shallow)\n",
    "fpr_deep, tpr_deep, _ = roc_curve(targets_test[:ntest_evts], predictions_deep)\n",
    "\n",
    "auc_shallow = auc(fpr_shallow, tpr_shallow)\n",
    "auc_deep = auc(fpr_deep, tpr_deep)\n",
    "\n",
    "print(\"AUC shallow model = \",auc_shallow)\n",
    "print(\"AUC  deep model   = \",auc_deep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(tpr_deep, 1.0-fpr_deep, lw=3, alpha=0.8,\n",
    "        label=\"Deep (AUC={:.2f})\".format(auc_deep))\n",
    "plt.plot(tpr_shallow, 1.0-fpr_shallow, lw=3, alpha=0.8,\n",
    "        label=\"Shallow (AUC={:.2f})\".format(auc_shallow))\n",
    "plt.xlabel(\"Signal efficiency\")\n",
    "plt.ylabel(\"Background rejection\")\n",
    "plt.legend(loc=3)\n",
    "plt.xlim((0.0, 1.0))\n",
    "plt.ylim((0.0, 1.0))\n",
    "plt.savefig(\"HIGGS_roc.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
